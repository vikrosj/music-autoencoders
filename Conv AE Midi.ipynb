{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A convolutional autoencoder\n",
    "\n",
    "Interesting note:\n",
    "This model performs better on representing the data than the MLP AE. \n",
    "It is less sensitive to overfitting, because of the Dropout layers. \n",
    "\n",
    "When comparing the mean and standard deviation of input data and reproduced data, \n",
    "this model comes much closer to the original than the MLP does.\n",
    "\n",
    "\n",
    "The model looks like this:\n",
    "\n",
    "```_______________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input_1 (InputLayer)         (None, 32, 1)             0         \n",
    "_________________________________________________________________\n",
    "conv1d_1 (Conv1D)            (None, 32, 18)            54        \n",
    "_________________________________________________________________\n",
    "max_pooling1d_1 (MaxPooling1 (None, 16, 18)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_2 (Conv1D)            (None, 16, 8)             296       \n",
    "_________________________________________________________________\n",
    "max_pooling1d_2 (MaxPooling1 (None, 8, 8)              0         \n",
    "_________________________________________________________________\n",
    "conv1d_3 (Conv1D)            (None, 8, 8)              136       \n",
    "_________________________________________________________________\n",
    "max_pooling1d_3 (MaxPooling1 (None, 4, 8)              0         \n",
    "_________________________________________________________________\n",
    "conv1d_4 (Conv1D)            (None, 4, 8)              136       \n",
    "_________________________________________________________________\n",
    "up_sampling1d_1 (UpSampling1 (None, 8, 8)              0         \n",
    "_________________________________________________________________\n",
    "conv1d_5 (Conv1D)            (None, 8, 8)              136       \n",
    "_________________________________________________________________\n",
    "up_sampling1d_2 (UpSampling1 (None, 16, 8)             0         \n",
    "_________________________________________________________________\n",
    "conv1d_6 (Conv1D)            (None, 16, 18)            306       \n",
    "_________________________________________________________________\n",
    "up_sampling1d_3 (UpSampling1 (None, 32, 18)            0         \n",
    "_________________________________________________________________\n",
    "conv1d_7 (Conv1D)            (None, 32, 1)             37        \n",
    "=================================================================\n",
    "Total params: 1,101\n",
    "Trainable params: 1,101\n",
    "Non-trainable params: 0```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, UpSampling1D\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from music21 import midi, stream\n",
    "\n",
    "# Importing functions used in AE Midi\n",
    "from create_m21_object import createMusic21Object\n",
    "from create_pattern import createPattern\n",
    "\n",
    "from music21 import converter, instrument, note, chord\n",
    "from music21.instrument import Guitar\n",
    "from music21 import midi, stream\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# For training\n",
    "epochs = 400\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11292, 32, 1), (5645, 32, 1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get data\n",
    "data = np.load(\"music.npz\")\n",
    "x_train = data['x_train']\n",
    "x_test = data['x_test']\n",
    "\n",
    "# Reshaping to use in a Conv1D network\n",
    "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
    "x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_song = Input(shape=(32,1)) # channels last"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder model\n",
    "\n",
    "In the encoder model, each layer halfens the size of the input, ending up with a bottleneck of size 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create encoder model\n",
    "\n",
    "x = Conv1D(filters=18, kernel_size=(2), activation='relu', padding='same')(input_song)\n",
    "x = MaxPooling1D((2), padding='same')(x)\n",
    "x = Conv1D(filters=8, kernel_size=(2), activation='relu', padding='same')(x)\n",
    "x = MaxPooling1D((2), padding='same')(x)\n",
    "x = Conv1D(8, (2), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling1D((2), padding='same')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder model\n",
    "\n",
    "In the decoder model, each layer doubles the size of its input, ending up with the output size equal to the input size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create decoder model\n",
    "\n",
    "x = Conv1D(8, (2), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling1D((2))(x)\n",
    "x = Conv1D(8, (2), activation='relu', padding='same')(x)\n",
    "x = UpSampling1D((2))(x)\n",
    "x = Conv1D(18, kernel_size=(2), activation='relu', padding='same')(x)\n",
    "x = UpSampling1D((2))(x)\n",
    "decoded = Conv1D(1, (2), activation='sigmoid', padding='same')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9033 samples, validate on 2259 samples\n",
      "Epoch 1/400\n",
      "9033/9033 [==============================] - 1s 159us/step - loss: 0.0350 - val_loss: 0.0397\n",
      "Epoch 2/400\n",
      "9033/9033 [==============================] - 1s 113us/step - loss: 0.0349 - val_loss: 0.0412\n",
      "Epoch 3/400\n",
      "9033/9033 [==============================] - 1s 118us/step - loss: 0.0349 - val_loss: 0.0407\n",
      "Epoch 4/400\n",
      "9033/9033 [==============================] - 1s 112us/step - loss: 0.0349 - val_loss: 0.0410\n",
      "Epoch 5/400\n",
      "9033/9033 [==============================] - 1s 115us/step - loss: 0.0348 - val_loss: 0.0416\n",
      "Epoch 6/400\n",
      "9033/9033 [==============================] - 1s 114us/step - loss: 0.0348 - val_loss: 0.0420\n",
      "Epoch 7/400\n",
      "9033/9033 [==============================] - 1s 115us/step - loss: 0.0348 - val_loss: 0.0399\n",
      "Epoch 8/400\n",
      "9033/9033 [==============================] - 1s 117us/step - loss: 0.0348 - val_loss: 0.0390\n",
      "Epoch 9/400\n",
      "9033/9033 [==============================] - 1s 118us/step - loss: 0.0348 - val_loss: 0.0400\n",
      "Epoch 10/400\n",
      "9033/9033 [==============================] - 1s 126us/step - loss: 0.0347 - val_loss: 0.0395\n",
      "Epoch 11/400\n",
      "9033/9033 [==============================] - 1s 133us/step - loss: 0.0348 - val_loss: 0.0394\n",
      "Epoch 12/400\n",
      "9033/9033 [==============================] - 1s 121us/step - loss: 0.0347 - val_loss: 0.0401\n",
      "Epoch 13/400\n",
      "9033/9033 [==============================] - 1s 116us/step - loss: 0.0347 - val_loss: 0.0392\n",
      "Epoch 14/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0347 - val_loss: 0.0391\n",
      "Epoch 15/400\n",
      "9033/9033 [==============================] - 1s 111us/step - loss: 0.0347 - val_loss: 0.0390\n",
      "Epoch 16/400\n",
      "9033/9033 [==============================] - 1s 130us/step - loss: 0.0347 - val_loss: 0.0391\n",
      "Epoch 17/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0346 - val_loss: 0.0392\n",
      "Epoch 18/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0346 - val_loss: 0.0392\n",
      "Epoch 19/400\n",
      "9033/9033 [==============================] - 1s 108us/step - loss: 0.0346 - val_loss: 0.0392\n",
      "Epoch 20/400\n",
      "9033/9033 [==============================] - 1s 99us/step - loss: 0.0346 - val_loss: 0.0389\n",
      "Epoch 21/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0346 - val_loss: 0.0417\n",
      "Epoch 22/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0346 - val_loss: 0.0390\n",
      "Epoch 23/400\n",
      "9033/9033 [==============================] - 1s 99us/step - loss: 0.0345 - val_loss: 0.0405\n",
      "Epoch 24/400\n",
      "9033/9033 [==============================] - 1s 99us/step - loss: 0.0345 - val_loss: 0.0390\n",
      "Epoch 25/400\n",
      "9033/9033 [==============================] - 1s 110us/step - loss: 0.0345 - val_loss: 0.0391\n",
      "Epoch 26/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0345 - val_loss: 0.0404\n",
      "Epoch 27/400\n",
      "9033/9033 [==============================] - 1s 105us/step - loss: 0.0345 - val_loss: 0.0410\n",
      "Epoch 28/400\n",
      "9033/9033 [==============================] - 1s 98us/step - loss: 0.0345 - val_loss: 0.0404\n",
      "Epoch 29/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0345 - val_loss: 0.0390\n",
      "Epoch 30/400\n",
      "9033/9033 [==============================] - 1s 104us/step - loss: 0.0344 - val_loss: 0.0402\n",
      "Epoch 31/400\n",
      "9033/9033 [==============================] - 1s 105us/step - loss: 0.0344 - val_loss: 0.0398\n",
      "Epoch 32/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0344 - val_loss: 0.0390\n",
      "Epoch 33/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0344 - val_loss: 0.0394\n",
      "Epoch 34/400\n",
      "9033/9033 [==============================] - 1s 104us/step - loss: 0.0344 - val_loss: 0.0388\n",
      "Epoch 35/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0343 - val_loss: 0.0393\n",
      "Epoch 36/400\n",
      "9033/9033 [==============================] - 1s 104us/step - loss: 0.0343 - val_loss: 0.0396\n",
      "Epoch 37/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0343 - val_loss: 0.0391\n",
      "Epoch 38/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0343 - val_loss: 0.0391\n",
      "Epoch 39/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0343 - val_loss: 0.0401\n",
      "Epoch 40/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0342 - val_loss: 0.0419\n",
      "Epoch 41/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0342 - val_loss: 0.0389\n",
      "Epoch 42/400\n",
      "9033/9033 [==============================] - 1s 98us/step - loss: 0.0342 - val_loss: 0.0392\n",
      "Epoch 43/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0342 - val_loss: 0.0398\n",
      "Epoch 44/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0342 - val_loss: 0.0392\n",
      "Epoch 45/400\n",
      "9033/9033 [==============================] - 1s 98us/step - loss: 0.0341 - val_loss: 0.0402\n",
      "Epoch 46/400\n",
      "9033/9033 [==============================] - 1s 99us/step - loss: 0.0342 - val_loss: 0.0402\n",
      "Epoch 47/400\n",
      "9033/9033 [==============================] - 1s 104us/step - loss: 0.0342 - val_loss: 0.0387\n",
      "Epoch 48/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0341 - val_loss: 0.0396\n",
      "Epoch 49/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0341 - val_loss: 0.0389\n",
      "Epoch 50/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0341 - val_loss: 0.0388\n",
      "Epoch 51/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0341 - val_loss: 0.0399\n",
      "Epoch 52/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0341 - val_loss: 0.0383\n",
      "Epoch 53/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0341 - val_loss: 0.0392\n",
      "Epoch 54/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0340 - val_loss: 0.0387\n",
      "Epoch 55/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0340 - val_loss: 0.0386\n",
      "Epoch 56/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0340 - val_loss: 0.0391\n",
      "Epoch 57/400\n",
      "9033/9033 [==============================] - 1s 105us/step - loss: 0.0340 - val_loss: 0.0386\n",
      "Epoch 58/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0340 - val_loss: 0.0391\n",
      "Epoch 59/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0339 - val_loss: 0.0385\n",
      "Epoch 60/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0340 - val_loss: 0.0388\n",
      "Epoch 61/400\n",
      "9033/9033 [==============================] - 1s 96us/step - loss: 0.0339 - val_loss: 0.0415\n",
      "Epoch 62/400\n",
      "9033/9033 [==============================] - 1s 99us/step - loss: 0.0339 - val_loss: 0.0393\n",
      "Epoch 63/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0338 - val_loss: 0.0415\n",
      "Epoch 64/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0339 - val_loss: 0.0391\n",
      "Epoch 65/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0339 - val_loss: 0.0406\n",
      "Epoch 66/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0338 - val_loss: 0.0384\n",
      "Epoch 67/400\n",
      "9033/9033 [==============================] - 1s 104us/step - loss: 0.0338 - val_loss: 0.0408\n",
      "Epoch 68/400\n",
      "9033/9033 [==============================] - 1s 98us/step - loss: 0.0338 - val_loss: 0.0394\n",
      "Epoch 69/400\n",
      "9033/9033 [==============================] - 1s 104us/step - loss: 0.0338 - val_loss: 0.0400\n",
      "Epoch 70/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0338 - val_loss: 0.0411\n",
      "Epoch 71/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0337 - val_loss: 0.0420\n",
      "Epoch 72/400\n",
      "9033/9033 [==============================] - 1s 106us/step - loss: 0.0338 - val_loss: 0.0387\n",
      "Epoch 73/400\n",
      "9033/9033 [==============================] - 1s 99us/step - loss: 0.0337 - val_loss: 0.0399\n",
      "Epoch 74/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0337 - val_loss: 0.0384\n",
      "Epoch 75/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0338 - val_loss: 0.0387\n",
      "Epoch 76/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0337 - val_loss: 0.0386\n",
      "Epoch 77/400\n",
      "9033/9033 [==============================] - 1s 104us/step - loss: 0.0337 - val_loss: 0.0383\n",
      "Epoch 78/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0337 - val_loss: 0.0385\n",
      "Epoch 79/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0337 - val_loss: 0.0388\n",
      "Epoch 80/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0336 - val_loss: 0.0380\n",
      "Epoch 81/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0337 - val_loss: 0.0387\n",
      "Epoch 82/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0336 - val_loss: 0.0401\n",
      "Epoch 83/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0336 - val_loss: 0.0382\n",
      "Epoch 84/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0336 - val_loss: 0.0390\n",
      "Epoch 85/400\n",
      "9033/9033 [==============================] - 1s 104us/step - loss: 0.0336 - val_loss: 0.0384\n",
      "Epoch 86/400\n",
      "9033/9033 [==============================] - 1s 99us/step - loss: 0.0336 - val_loss: 0.0382\n",
      "Epoch 87/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0336 - val_loss: 0.0390\n",
      "Epoch 88/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0336 - val_loss: 0.0412\n",
      "Epoch 89/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0335 - val_loss: 0.0382\n",
      "Epoch 90/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0335 - val_loss: 0.0387\n",
      "Epoch 91/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0335 - val_loss: 0.0392\n",
      "Epoch 92/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0335 - val_loss: 0.0385\n",
      "Epoch 93/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0335 - val_loss: 0.0387\n",
      "Epoch 94/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0335 - val_loss: 0.0415\n",
      "Epoch 95/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0335 - val_loss: 0.0395\n",
      "Epoch 96/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0335 - val_loss: 0.0394\n",
      "Epoch 97/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0335 - val_loss: 0.0382\n",
      "Epoch 98/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0335 - val_loss: 0.0379\n",
      "Epoch 99/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0334 - val_loss: 0.0382\n",
      "Epoch 100/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0334 - val_loss: 0.0406\n",
      "Epoch 101/400\n",
      "9033/9033 [==============================] - 1s 99us/step - loss: 0.0334 - val_loss: 0.0420\n",
      "Epoch 102/400\n",
      "9033/9033 [==============================] - 1s 105us/step - loss: 0.0334 - val_loss: 0.0388\n",
      "Epoch 103/400\n",
      "9033/9033 [==============================] - 1s 95us/step - loss: 0.0334 - val_loss: 0.0393\n",
      "Epoch 104/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0334 - val_loss: 0.0379\n",
      "Epoch 105/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0334 - val_loss: 0.0384\n",
      "Epoch 106/400\n",
      "9033/9033 [==============================] - 1s 99us/step - loss: 0.0333 - val_loss: 0.0390\n",
      "Epoch 107/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0333 - val_loss: 0.0379\n",
      "Epoch 108/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0333 - val_loss: 0.0401\n",
      "Epoch 109/400\n",
      "9033/9033 [==============================] - 1s 104us/step - loss: 0.0333 - val_loss: 0.0381\n",
      "Epoch 110/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0333 - val_loss: 0.0384\n",
      "Epoch 111/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0333 - val_loss: 0.0383\n",
      "Epoch 112/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0333 - val_loss: 0.0381\n",
      "Epoch 113/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0333 - val_loss: 0.0381\n",
      "Epoch 114/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0332 - val_loss: 0.0400\n",
      "Epoch 115/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0333 - val_loss: 0.0404\n",
      "Epoch 116/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0332 - val_loss: 0.0381\n",
      "Epoch 117/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0332 - val_loss: 0.0386\n",
      "Epoch 118/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0332 - val_loss: 0.0379\n",
      "Epoch 119/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0333 - val_loss: 0.0380\n",
      "Epoch 120/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0332 - val_loss: 0.0377\n",
      "Epoch 121/400\n",
      "9033/9033 [==============================] - 1s 104us/step - loss: 0.0332 - val_loss: 0.0381\n",
      "Epoch 122/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0332 - val_loss: 0.0387\n",
      "Epoch 123/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0332 - val_loss: 0.0386\n",
      "Epoch 124/400\n",
      "9033/9033 [==============================] - 1s 99us/step - loss: 0.0331 - val_loss: 0.0408\n",
      "Epoch 125/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0331 - val_loss: 0.0396\n",
      "Epoch 126/400\n",
      "9033/9033 [==============================] - 1s 97us/step - loss: 0.0332 - val_loss: 0.0400\n",
      "Epoch 127/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0331 - val_loss: 0.0381\n",
      "Epoch 128/400\n",
      "9033/9033 [==============================] - 1s 104us/step - loss: 0.0331 - val_loss: 0.0378\n",
      "Epoch 129/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0331 - val_loss: 0.0385\n",
      "Epoch 130/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0330 - val_loss: 0.0380\n",
      "Epoch 131/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0330 - val_loss: 0.0380\n",
      "Epoch 132/400\n",
      "9033/9033 [==============================] - 1s 98us/step - loss: 0.0331 - val_loss: 0.0400\n",
      "Epoch 133/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0330 - val_loss: 0.0384\n",
      "Epoch 134/400\n",
      "9033/9033 [==============================] - 1s 98us/step - loss: 0.0330 - val_loss: 0.0378\n",
      "Epoch 135/400\n",
      "9033/9033 [==============================] - 1s 98us/step - loss: 0.0330 - val_loss: 0.0386\n",
      "Epoch 136/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0330 - val_loss: 0.0383\n",
      "Epoch 137/400\n",
      "9033/9033 [==============================] - 1s 98us/step - loss: 0.0330 - val_loss: 0.0374\n",
      "Epoch 138/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0329 - val_loss: 0.0386\n",
      "Epoch 139/400\n",
      "9033/9033 [==============================] - 1s 106us/step - loss: 0.0330 - val_loss: 0.0376\n",
      "Epoch 140/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0330 - val_loss: 0.0377\n",
      "Epoch 141/400\n",
      "9033/9033 [==============================] - 1s 104us/step - loss: 0.0329 - val_loss: 0.0385\n",
      "Epoch 142/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0329 - val_loss: 0.0388\n",
      "Epoch 143/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0329 - val_loss: 0.0378\n",
      "Epoch 144/400\n",
      "9033/9033 [==============================] - 1s 105us/step - loss: 0.0329 - val_loss: 0.0389\n",
      "Epoch 145/400\n",
      "9033/9033 [==============================] - 1s 104us/step - loss: 0.0329 - val_loss: 0.0382\n",
      "Epoch 146/400\n",
      "9033/9033 [==============================] - 1s 105us/step - loss: 0.0329 - val_loss: 0.0379\n",
      "Epoch 147/400\n",
      "9033/9033 [==============================] - 1s 98us/step - loss: 0.0329 - val_loss: 0.0382\n",
      "Epoch 148/400\n",
      "9033/9033 [==============================] - 1s 99us/step - loss: 0.0328 - val_loss: 0.0383\n",
      "Epoch 149/400\n",
      "9033/9033 [==============================] - 1s 99us/step - loss: 0.0328 - val_loss: 0.0378\n",
      "Epoch 150/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0328 - val_loss: 0.0383\n",
      "Epoch 151/400\n",
      "9033/9033 [==============================] - 1s 111us/step - loss: 0.0328 - val_loss: 0.0378\n",
      "Epoch 152/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0328 - val_loss: 0.0376\n",
      "Epoch 153/400\n",
      "9033/9033 [==============================] - 1s 105us/step - loss: 0.0327 - val_loss: 0.0373\n",
      "Epoch 154/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0327 - val_loss: 0.0390\n",
      "Epoch 155/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0327 - val_loss: 0.0376\n",
      "Epoch 156/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0327 - val_loss: 0.0383\n",
      "Epoch 157/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0327 - val_loss: 0.0372\n",
      "Epoch 158/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0326 - val_loss: 0.0389\n",
      "Epoch 159/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0326 - val_loss: 0.0376\n",
      "Epoch 160/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0326 - val_loss: 0.0373\n",
      "Epoch 161/400\n",
      "9033/9033 [==============================] - 1s 99us/step - loss: 0.0326 - val_loss: 0.0378\n",
      "Epoch 162/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0326 - val_loss: 0.0388\n",
      "Epoch 163/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0325 - val_loss: 0.0371\n",
      "Epoch 164/400\n",
      "9033/9033 [==============================] - 1s 99us/step - loss: 0.0326 - val_loss: 0.0377\n",
      "Epoch 165/400\n",
      "9033/9033 [==============================] - 1s 104us/step - loss: 0.0325 - val_loss: 0.0385\n",
      "Epoch 166/400\n",
      "9033/9033 [==============================] - 1s 98us/step - loss: 0.0325 - val_loss: 0.0379\n",
      "Epoch 167/400\n",
      "9033/9033 [==============================] - 1s 98us/step - loss: 0.0324 - val_loss: 0.0371\n",
      "Epoch 168/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0324 - val_loss: 0.0411\n",
      "Epoch 169/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0325 - val_loss: 0.0371\n",
      "Epoch 170/400\n",
      "9033/9033 [==============================] - 1s 104us/step - loss: 0.0324 - val_loss: 0.0379\n",
      "Epoch 171/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0324 - val_loss: 0.0367\n",
      "Epoch 172/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0324 - val_loss: 0.0379\n",
      "Epoch 173/400\n",
      "9033/9033 [==============================] - 1s 107us/step - loss: 0.0324 - val_loss: 0.0377\n",
      "Epoch 174/400\n",
      "9033/9033 [==============================] - 1s 98us/step - loss: 0.0323 - val_loss: 0.0377\n",
      "Epoch 175/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0323 - val_loss: 0.0372\n",
      "Epoch 176/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0323 - val_loss: 0.0373\n",
      "Epoch 177/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0323 - val_loss: 0.0375\n",
      "Epoch 178/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0322 - val_loss: 0.0379\n",
      "Epoch 179/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0322 - val_loss: 0.0365\n",
      "Epoch 180/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0322 - val_loss: 0.0381\n",
      "Epoch 181/400\n",
      "9033/9033 [==============================] - 1s 99us/step - loss: 0.0322 - val_loss: 0.0378\n",
      "Epoch 182/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0322 - val_loss: 0.0366\n",
      "Epoch 183/400\n",
      "9033/9033 [==============================] - 1s 105us/step - loss: 0.0322 - val_loss: 0.0379\n",
      "Epoch 184/400\n",
      "9033/9033 [==============================] - 1s 99us/step - loss: 0.0321 - val_loss: 0.0379\n",
      "Epoch 185/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0321 - val_loss: 0.0401\n",
      "Epoch 186/400\n",
      "9033/9033 [==============================] - 1s 104us/step - loss: 0.0321 - val_loss: 0.0369\n",
      "Epoch 187/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0321 - val_loss: 0.0367\n",
      "Epoch 188/400\n",
      "9033/9033 [==============================] - 1s 104us/step - loss: 0.0321 - val_loss: 0.0383\n",
      "Epoch 189/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0320 - val_loss: 0.0395\n",
      "Epoch 190/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0321 - val_loss: 0.0403\n",
      "Epoch 191/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0320 - val_loss: 0.0366\n",
      "Epoch 192/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0320 - val_loss: 0.0377\n",
      "Epoch 193/400\n",
      "9033/9033 [==============================] - 1s 105us/step - loss: 0.0320 - val_loss: 0.0372\n",
      "Epoch 194/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0320 - val_loss: 0.0366\n",
      "Epoch 195/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0320 - val_loss: 0.0372\n",
      "Epoch 196/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0319 - val_loss: 0.0388\n",
      "Epoch 197/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0320 - val_loss: 0.0374\n",
      "Epoch 198/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0319 - val_loss: 0.0380\n",
      "Epoch 199/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0319 - val_loss: 0.0381\n",
      "Epoch 200/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0319 - val_loss: 0.0372\n",
      "Epoch 201/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0319 - val_loss: 0.0362\n",
      "Epoch 202/400\n",
      "9033/9033 [==============================] - 1s 104us/step - loss: 0.0318 - val_loss: 0.0366\n",
      "Epoch 203/400\n",
      "9033/9033 [==============================] - 1s 104us/step - loss: 0.0318 - val_loss: 0.0370\n",
      "Epoch 204/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0318 - val_loss: 0.0373\n",
      "Epoch 205/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0318 - val_loss: 0.0364\n",
      "Epoch 206/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0318 - val_loss: 0.0363\n",
      "Epoch 207/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0318 - val_loss: 0.0371\n",
      "Epoch 208/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0318 - val_loss: 0.0366\n",
      "Epoch 209/400\n",
      "9033/9033 [==============================] - 1s 99us/step - loss: 0.0317 - val_loss: 0.0361\n",
      "Epoch 210/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0317 - val_loss: 0.0362\n",
      "Epoch 211/400\n",
      "9033/9033 [==============================] - 1s 112us/step - loss: 0.0317 - val_loss: 0.0386\n",
      "Epoch 212/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0317 - val_loss: 0.0362\n",
      "Epoch 213/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0317 - val_loss: 0.0361\n",
      "Epoch 214/400\n",
      "9033/9033 [==============================] - 1s 99us/step - loss: 0.0316 - val_loss: 0.0375\n",
      "Epoch 215/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0317 - val_loss: 0.0368\n",
      "Epoch 216/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0316 - val_loss: 0.0363\n",
      "Epoch 217/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0316 - val_loss: 0.0392\n",
      "Epoch 218/400\n",
      "9033/9033 [==============================] - 1s 98us/step - loss: 0.0316 - val_loss: 0.0373\n",
      "Epoch 219/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0316 - val_loss: 0.0372\n",
      "Epoch 220/400\n",
      "9033/9033 [==============================] - 1s 110us/step - loss: 0.0316 - val_loss: 0.0378\n",
      "Epoch 221/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0316 - val_loss: 0.0381\n",
      "Epoch 222/400\n",
      "9033/9033 [==============================] - 1s 98us/step - loss: 0.0315 - val_loss: 0.0387\n",
      "Epoch 223/400\n",
      "9033/9033 [==============================] - 1s 99us/step - loss: 0.0316 - val_loss: 0.0373\n",
      "Epoch 224/400\n",
      "9033/9033 [==============================] - 1s 104us/step - loss: 0.0315 - val_loss: 0.0366\n",
      "Epoch 225/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0315 - val_loss: 0.0368\n",
      "Epoch 226/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0315 - val_loss: 0.0376\n",
      "Epoch 227/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0315 - val_loss: 0.0361\n",
      "Epoch 228/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0315 - val_loss: 0.0362\n",
      "Epoch 229/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0315 - val_loss: 0.0369\n",
      "Epoch 230/400\n",
      "9033/9033 [==============================] - 1s 104us/step - loss: 0.0314 - val_loss: 0.0365\n",
      "Epoch 231/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0314 - val_loss: 0.0360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 232/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0314 - val_loss: 0.0365\n",
      "Epoch 233/400\n",
      "9033/9033 [==============================] - 1s 105us/step - loss: 0.0314 - val_loss: 0.0377\n",
      "Epoch 234/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0314 - val_loss: 0.0384\n",
      "Epoch 235/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0314 - val_loss: 0.0357\n",
      "Epoch 236/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0314 - val_loss: 0.0370\n",
      "Epoch 237/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0313 - val_loss: 0.0365\n",
      "Epoch 238/400\n",
      "9033/9033 [==============================] - 1s 104us/step - loss: 0.0313 - val_loss: 0.0378\n",
      "Epoch 239/400\n",
      "9033/9033 [==============================] - 1s 106us/step - loss: 0.0313 - val_loss: 0.0370\n",
      "Epoch 240/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0313 - val_loss: 0.0365\n",
      "Epoch 241/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0313 - val_loss: 0.0359\n",
      "Epoch 242/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0312 - val_loss: 0.0364\n",
      "Epoch 243/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0312 - val_loss: 0.0366\n",
      "Epoch 244/400\n",
      "9033/9033 [==============================] - 1s 99us/step - loss: 0.0312 - val_loss: 0.0392\n",
      "Epoch 245/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0313 - val_loss: 0.0397\n",
      "Epoch 246/400\n",
      "9033/9033 [==============================] - 1s 98us/step - loss: 0.0312 - val_loss: 0.0360\n",
      "Epoch 247/400\n",
      "9033/9033 [==============================] - 1s 106us/step - loss: 0.0312 - val_loss: 0.0357\n",
      "Epoch 248/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0311 - val_loss: 0.0375\n",
      "Epoch 249/400\n",
      "9033/9033 [==============================] - 1s 99us/step - loss: 0.0311 - val_loss: 0.0366\n",
      "Epoch 250/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0312 - val_loss: 0.0361\n",
      "Epoch 251/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0311 - val_loss: 0.0364\n",
      "Epoch 252/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0311 - val_loss: 0.0365\n",
      "Epoch 253/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0311 - val_loss: 0.0366\n",
      "Epoch 254/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0311 - val_loss: 0.0362\n",
      "Epoch 255/400\n",
      "9033/9033 [==============================] - 1s 104us/step - loss: 0.0311 - val_loss: 0.0358\n",
      "Epoch 256/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0311 - val_loss: 0.0360\n",
      "Epoch 257/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0310 - val_loss: 0.0356\n",
      "Epoch 258/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0310 - val_loss: 0.0369\n",
      "Epoch 259/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0310 - val_loss: 0.0363\n",
      "Epoch 260/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0310 - val_loss: 0.0360\n",
      "Epoch 261/400\n",
      "9033/9033 [==============================] - 1s 104us/step - loss: 0.0310 - val_loss: 0.0365\n",
      "Epoch 262/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0310 - val_loss: 0.0359\n",
      "Epoch 263/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0310 - val_loss: 0.0374\n",
      "Epoch 264/400\n",
      "9033/9033 [==============================] - 1s 99us/step - loss: 0.0309 - val_loss: 0.0366\n",
      "Epoch 265/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0309 - val_loss: 0.0359\n",
      "Epoch 266/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0309 - val_loss: 0.0358\n",
      "Epoch 267/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0308 - val_loss: 0.0378\n",
      "Epoch 268/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0309 - val_loss: 0.0367\n",
      "Epoch 269/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0309 - val_loss: 0.0363\n",
      "Epoch 270/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0309 - val_loss: 0.0369\n",
      "Epoch 271/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0309 - val_loss: 0.0375\n",
      "Epoch 272/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0309 - val_loss: 0.0366\n",
      "Epoch 273/400\n",
      "9033/9033 [==============================] - 1s 107us/step - loss: 0.0308 - val_loss: 0.0357\n",
      "Epoch 274/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0308 - val_loss: 0.0364\n",
      "Epoch 275/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0308 - val_loss: 0.0361\n",
      "Epoch 276/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0308 - val_loss: 0.0377\n",
      "Epoch 277/400\n",
      "9033/9033 [==============================] - 1s 104us/step - loss: 0.0308 - val_loss: 0.0379\n",
      "Epoch 278/400\n",
      "9033/9033 [==============================] - 1s 104us/step - loss: 0.0308 - val_loss: 0.0372\n",
      "Epoch 279/400\n",
      "9033/9033 [==============================] - 1s 104us/step - loss: 0.0307 - val_loss: 0.0354\n",
      "Epoch 280/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0307 - val_loss: 0.0357\n",
      "Epoch 281/400\n",
      "9033/9033 [==============================] - 1s 99us/step - loss: 0.0308 - val_loss: 0.0354\n",
      "Epoch 282/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0307 - val_loss: 0.0368\n",
      "Epoch 283/400\n",
      "9033/9033 [==============================] - 1s 99us/step - loss: 0.0307 - val_loss: 0.0366\n",
      "Epoch 284/400\n",
      "9033/9033 [==============================] - 1s 104us/step - loss: 0.0307 - val_loss: 0.0353\n",
      "Epoch 285/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0307 - val_loss: 0.0376\n",
      "Epoch 286/400\n",
      "9033/9033 [==============================] - 1s 105us/step - loss: 0.0307 - val_loss: 0.0361\n",
      "Epoch 287/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0307 - val_loss: 0.0377\n",
      "Epoch 288/400\n",
      "9033/9033 [==============================] - 1s 98us/step - loss: 0.0307 - val_loss: 0.0357\n",
      "Epoch 289/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0306 - val_loss: 0.0378\n",
      "Epoch 290/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0306 - val_loss: 0.0362\n",
      "Epoch 291/400\n",
      "9033/9033 [==============================] - 1s 99us/step - loss: 0.0306 - val_loss: 0.0379\n",
      "Epoch 292/400\n",
      "9033/9033 [==============================] - 1s 104us/step - loss: 0.0306 - val_loss: 0.0360\n",
      "Epoch 293/400\n",
      "9033/9033 [==============================] - 1s 99us/step - loss: 0.0306 - val_loss: 0.0362\n",
      "Epoch 294/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0306 - val_loss: 0.0356\n",
      "Epoch 295/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0306 - val_loss: 0.0351\n",
      "Epoch 296/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0306 - val_loss: 0.0364\n",
      "Epoch 297/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0306 - val_loss: 0.0372\n",
      "Epoch 298/400\n",
      "9033/9033 [==============================] - 1s 105us/step - loss: 0.0306 - val_loss: 0.0354\n",
      "Epoch 299/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0306 - val_loss: 0.0366\n",
      "Epoch 300/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0305 - val_loss: 0.0356\n",
      "Epoch 301/400\n",
      "9033/9033 [==============================] - 1s 104us/step - loss: 0.0306 - val_loss: 0.0352\n",
      "Epoch 302/400\n",
      "9033/9033 [==============================] - 1s 98us/step - loss: 0.0305 - val_loss: 0.0357\n",
      "Epoch 303/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0305 - val_loss: 0.0355\n",
      "Epoch 304/400\n",
      "9033/9033 [==============================] - 1s 107us/step - loss: 0.0305 - val_loss: 0.0361\n",
      "Epoch 305/400\n",
      "9033/9033 [==============================] - 1s 105us/step - loss: 0.0305 - val_loss: 0.0352\n",
      "Epoch 306/400\n",
      "9033/9033 [==============================] - 1s 104us/step - loss: 0.0305 - val_loss: 0.0354\n",
      "Epoch 307/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0304 - val_loss: 0.0398\n",
      "Epoch 308/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0305 - val_loss: 0.0351\n",
      "Epoch 309/400\n",
      "9033/9033 [==============================] - 1s 99us/step - loss: 0.0305 - val_loss: 0.0355\n",
      "Epoch 310/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0304 - val_loss: 0.0363\n",
      "Epoch 311/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0305 - val_loss: 0.0350\n",
      "Epoch 312/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0304 - val_loss: 0.0357\n",
      "Epoch 313/400\n",
      "9033/9033 [==============================] - 1s 105us/step - loss: 0.0304 - val_loss: 0.0362\n",
      "Epoch 314/400\n",
      "9033/9033 [==============================] - 1s 99us/step - loss: 0.0305 - val_loss: 0.0350\n",
      "Epoch 315/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0304 - val_loss: 0.0353\n",
      "Epoch 316/400\n",
      "9033/9033 [==============================] - 1s 105us/step - loss: 0.0304 - val_loss: 0.0372\n",
      "Epoch 317/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0304 - val_loss: 0.0372\n",
      "Epoch 318/400\n",
      "9033/9033 [==============================] - 1s 99us/step - loss: 0.0304 - val_loss: 0.0376\n",
      "Epoch 319/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0304 - val_loss: 0.0359\n",
      "Epoch 320/400\n",
      "9033/9033 [==============================] - 1s 105us/step - loss: 0.0304 - val_loss: 0.0367\n",
      "Epoch 321/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0304 - val_loss: 0.0367\n",
      "Epoch 322/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0304 - val_loss: 0.0355\n",
      "Epoch 323/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0304 - val_loss: 0.0351\n",
      "Epoch 324/400\n",
      "9033/9033 [==============================] - 1s 99us/step - loss: 0.0304 - val_loss: 0.0352\n",
      "Epoch 325/400\n",
      "9033/9033 [==============================] - 1s 106us/step - loss: 0.0303 - val_loss: 0.0358\n",
      "Epoch 326/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0303 - val_loss: 0.0382\n",
      "Epoch 327/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0303 - val_loss: 0.0348\n",
      "Epoch 328/400\n",
      "9033/9033 [==============================] - 1s 104us/step - loss: 0.0303 - val_loss: 0.0353\n",
      "Epoch 329/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0303 - val_loss: 0.0368\n",
      "Epoch 330/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0303 - val_loss: 0.0381\n",
      "Epoch 331/400\n",
      "9033/9033 [==============================] - 1s 99us/step - loss: 0.0303 - val_loss: 0.0353\n",
      "Epoch 332/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0303 - val_loss: 0.0362\n",
      "Epoch 333/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0303 - val_loss: 0.0364\n",
      "Epoch 334/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0303 - val_loss: 0.0349\n",
      "Epoch 335/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0303 - val_loss: 0.0353\n",
      "Epoch 336/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0303 - val_loss: 0.0350\n",
      "Epoch 337/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0303 - val_loss: 0.0351\n",
      "Epoch 338/400\n",
      "9033/9033 [==============================] - 1s 99us/step - loss: 0.0302 - val_loss: 0.0352\n",
      "Epoch 339/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0303 - val_loss: 0.0363\n",
      "Epoch 340/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0303 - val_loss: 0.0361\n",
      "Epoch 341/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0303 - val_loss: 0.0357\n",
      "Epoch 342/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0302 - val_loss: 0.0371\n",
      "Epoch 343/400\n",
      "9033/9033 [==============================] - 1s 104us/step - loss: 0.0302 - val_loss: 0.0357\n",
      "Epoch 344/400\n",
      "9033/9033 [==============================] - 1s 105us/step - loss: 0.0302 - val_loss: 0.0375\n",
      "Epoch 345/400\n",
      "9033/9033 [==============================] - 1s 104us/step - loss: 0.0302 - val_loss: 0.0359\n",
      "Epoch 346/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0302 - val_loss: 0.0358\n",
      "Epoch 347/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0302 - val_loss: 0.0347\n",
      "Epoch 348/400\n",
      "9033/9033 [==============================] - 1s 98us/step - loss: 0.0302 - val_loss: 0.0354\n",
      "Epoch 349/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0301 - val_loss: 0.0362\n",
      "Epoch 350/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0302 - val_loss: 0.0351\n",
      "Epoch 351/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0302 - val_loss: 0.0349\n",
      "Epoch 352/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0302 - val_loss: 0.0353\n",
      "Epoch 353/400\n",
      "9033/9033 [==============================] - 1s 98us/step - loss: 0.0301 - val_loss: 0.0360\n",
      "Epoch 354/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0301 - val_loss: 0.0353\n",
      "Epoch 355/400\n",
      "9033/9033 [==============================] - 1s 99us/step - loss: 0.0301 - val_loss: 0.0392\n",
      "Epoch 356/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0301 - val_loss: 0.0348\n",
      "Epoch 357/400\n",
      "9033/9033 [==============================] - 1s 104us/step - loss: 0.0301 - val_loss: 0.0355\n",
      "Epoch 358/400\n",
      "9033/9033 [==============================] - 1s 97us/step - loss: 0.0301 - val_loss: 0.0356\n",
      "Epoch 359/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0301 - val_loss: 0.0364\n",
      "Epoch 360/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0301 - val_loss: 0.0375\n",
      "Epoch 361/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0301 - val_loss: 0.0348\n",
      "Epoch 362/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0301 - val_loss: 0.0397\n",
      "Epoch 363/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0301 - val_loss: 0.0354\n",
      "Epoch 364/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0301 - val_loss: 0.0367\n",
      "Epoch 365/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0301 - val_loss: 0.0393\n",
      "Epoch 366/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0301 - val_loss: 0.0344\n",
      "Epoch 367/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0300 - val_loss: 0.0351\n",
      "Epoch 368/400\n",
      "9033/9033 [==============================] - 1s 107us/step - loss: 0.0301 - val_loss: 0.0350\n",
      "Epoch 369/400\n",
      "9033/9033 [==============================] - 1s 99us/step - loss: 0.0301 - val_loss: 0.0349\n",
      "Epoch 370/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0300 - val_loss: 0.0355\n",
      "Epoch 371/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0300 - val_loss: 0.0377\n",
      "Epoch 372/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0300 - val_loss: 0.0352\n",
      "Epoch 373/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0300 - val_loss: 0.0347\n",
      "Epoch 374/400\n",
      "9033/9033 [==============================] - 1s 98us/step - loss: 0.0300 - val_loss: 0.0349\n",
      "Epoch 375/400\n",
      "9033/9033 [==============================] - 1s 104us/step - loss: 0.0300 - val_loss: 0.0360\n",
      "Epoch 376/400\n",
      "9033/9033 [==============================] - 1s 105us/step - loss: 0.0300 - val_loss: 0.0353\n",
      "Epoch 377/400\n",
      "9033/9033 [==============================] - 1s 106us/step - loss: 0.0300 - val_loss: 0.0376\n",
      "Epoch 378/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0300 - val_loss: 0.0362\n",
      "Epoch 379/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0300 - val_loss: 0.0360\n",
      "Epoch 380/400\n",
      "9033/9033 [==============================] - 1s 98us/step - loss: 0.0300 - val_loss: 0.0343\n",
      "Epoch 381/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0300 - val_loss: 0.0362\n",
      "Epoch 382/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0300 - val_loss: 0.0359\n",
      "Epoch 383/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0299 - val_loss: 0.0366\n",
      "Epoch 384/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0300 - val_loss: 0.0352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 385/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0300 - val_loss: 0.0350\n",
      "Epoch 386/400\n",
      "9033/9033 [==============================] - 1s 100us/step - loss: 0.0299 - val_loss: 0.0346\n",
      "Epoch 387/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0299 - val_loss: 0.0359\n",
      "Epoch 388/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0299 - val_loss: 0.0346\n",
      "Epoch 389/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0299 - val_loss: 0.0384\n",
      "Epoch 390/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0299 - val_loss: 0.0353\n",
      "Epoch 391/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0299 - val_loss: 0.0347\n",
      "Epoch 392/400\n",
      "9033/9033 [==============================] - 1s 103us/step - loss: 0.0299 - val_loss: 0.0349\n",
      "Epoch 393/400\n",
      "9033/9033 [==============================] - 1s 105us/step - loss: 0.0299 - val_loss: 0.0359\n",
      "Epoch 394/400\n",
      "9033/9033 [==============================] - 1s 105us/step - loss: 0.0299 - val_loss: 0.0352\n",
      "Epoch 395/400\n",
      "9033/9033 [==============================] - 1s 104us/step - loss: 0.0299 - val_loss: 0.0348\n",
      "Epoch 396/400\n",
      "9033/9033 [==============================] - 1s 104us/step - loss: 0.0299 - val_loss: 0.0365\n",
      "Epoch 397/400\n",
      "9033/9033 [==============================] - 1s 105us/step - loss: 0.0298 - val_loss: 0.0352\n",
      "Epoch 398/400\n",
      "9033/9033 [==============================] - 1s 98us/step - loss: 0.0299 - val_loss: 0.0362\n",
      "Epoch 399/400\n",
      "9033/9033 [==============================] - 1s 101us/step - loss: 0.0298 - val_loss: 0.0362\n",
      "Epoch 400/400\n",
      "9033/9033 [==============================] - 1s 102us/step - loss: 0.0298 - val_loss: 0.0346\n",
      "time to train 370.86094522476196\n"
     ]
    }
   ],
   "source": [
    "autoencoder = Model(input_song, decoded)\n",
    "autoencoder.compile(optimizer='rmsprop', loss='mean_squared_error')\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=10e-5, patience=5,\n",
    "                          verbose=1, mode='min')\n",
    "\n",
    "callbacks_list = [earlystop]\n",
    "\n",
    "# train the model\n",
    "start = time.time()\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "               epochs=epochs,\n",
    "               batch_size=batch_size,\n",
    "               shuffle=True,\n",
    "               validation_split=0.2)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"time to train\", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing feature range from MLP example\n",
    "feature_range = np.load(\"feature_range.npy\")\n",
    "\n",
    "# Creating a scaler for the predicted values\n",
    "predictScaler = MinMaxScaler(feature_range=feature_range)\n",
    "\n",
    "song_prediction = autoencoder.predict(x_test)\n",
    "\n",
    "# Reshape to allow scaler to work\n",
    "song = song_prediction.reshape((song_prediction.shape[0], song_prediction.shape[1]))\n",
    "\n",
    "song = (predictScaler.fit_transform(song)).astype('int')\n",
    "newsong = createPattern(song[40])\n",
    "\n",
    "play = createMusic21Object(newsong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = midi.MidiTrack(0)\n",
    "dt = midi.DeltaTime(mt)\n",
    "dt.time = 0.5\n",
    "s1 = stream.Stream()\n",
    "\n",
    "offset = 0\n",
    "for item in newsong:\n",
    "    \n",
    "    if ('.' in item) or item.isdigit():\n",
    "        # chord\n",
    "        notes_in_chord = item.split('.')\n",
    "        notes = []\n",
    "\n",
    "        for current_note in notes_in_chord:\n",
    "            new_note = note.Note(int(current_note))\n",
    "            new_note.storedInstrument = instrument.Guitar()\n",
    "            new_note.offset = offset\n",
    "            notes.append(new_note)\n",
    "\n",
    "        new_chord = chord.Chord(notes)\n",
    "        s1.append(new_chord)\n",
    "\n",
    "    elif item is not '' and ('.' not in item):\n",
    "        # notes\n",
    "        new_note = note.Note(item)\n",
    "        s1.append(new_note)\n",
    "\n",
    "\n",
    "    elif item == '':\n",
    "        # rest\n",
    "        s1.append(note.Rest())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_stream = stream.Stream(s1)\n",
    "\n",
    "sp = midi.realtime.StreamPlayer(midi_stream)\n",
    "\n",
    "sp.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8418441, 0.8293800943207219, 0.19854255, 0.26314332054883544)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_prediction.mean(), x_test.mean(), song_prediction.std(), x_test.std()\n",
    "# autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magenta3",
   "language": "python",
   "name": "magenta3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
