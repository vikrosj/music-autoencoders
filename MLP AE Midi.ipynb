{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A fully connected autoencoder network\n",
    "\n",
    "That looks like this:\n",
    "\n",
    "```\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input_1 (InputLayer)         (None, 32)                0         \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 10)                660       \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 32)                672       \n",
    "=================================================================\n",
    "Total params: 1,332\n",
    "Trainable params: 1,332\n",
    "Non-trainable params: 0\n",
    "```\n",
    "\n",
    "The input layer takes a sequence of 32 notes and/or chords, the first Dense layer has 10 nodes, and hereby compress the input to a \"latent space\". The second Dense layer reconstruct the input from the latent space.\n",
    "\n",
    "Since it is a very small network, the bottleneck cannot be very much smaller than the input, at this level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vik/miniconda3/envs/magenta3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "# Keras\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "import keras.utils as utils\n",
    "# in case of need for activity regularizers\n",
    "from keras import regularizers\n",
    "# earlystopping prevents overfitting\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# For midi\n",
    "from music21 import converter, instrument, note, chord\n",
    "from music21.instrument import Guitar\n",
    "from music21 import midi, stream\n",
    "\n",
    "# To calculate training time\n",
    "import time\n",
    "\n",
    "# To create scaler for normalizing embedded chords/notes\n",
    "# and rescaling back to embedding after training\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=10e6)\n",
    "\n",
    "# bottleneck\n",
    "encoding_dim = 10\n",
    "\n",
    "# for training\n",
    "epochs = 400\n",
    "batch_size = 450"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset\n",
    "\n",
    "The notes for the dataset has been parsed in the notebook \"Midi Parsing\".\n",
    "The textfile contains a long string of notes and chords. \n",
    "\n",
    "Here, I split the string, and convert it to a list of strings.\n",
    "\n",
    "The last ten elements look like this:\n",
    "\n",
    "```\n",
    "['E2', '9.1.4', 'A2', 'E2', 'A2', 'E2', '9.1', '', '9.1']\n",
    "``` \n",
    "\n",
    "'A2', 'E2' etc. are notes, and their pitch.\n",
    "\n",
    "The numbers, e.g. '9.1.4' means three separate notes, played simultaneously - aka a chord.\n",
    "\n",
    "The empty text string '' means a note rest.\n",
    "\n",
    "This is a chord representation in their *normal order* - which is a concept I don't fully understand. It has something to do with semitone intervals.\n",
    "\n",
    "These are representations that are understood by the **music21** library as different chords.\n",
    "\n",
    "```len(notes) = 598820```\n",
    "\n",
    "so all the songs are compressed into a long sequence with length 598820"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset\n",
    "newTextfile = open('notesNew.txt', 'r')\n",
    "newNotes = newTextfile.readlines()\n",
    "newTextfile.close()\n",
    "\n",
    "notes = []\n",
    "for line in newNotes:\n",
    "    notes = line.split(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the dataset\n",
    "\n",
    "Here, I'm creating embeddings of all of the notes/chords. The embeddings and their notes/chords becomes a dictionary, called note_to_int.\n",
    "\n",
    "A snippet from note to int:\n",
    "\n",
    "```\n",
    "'9.11.2.3': 441,\n",
    " '9.11.2.4': 442,\n",
    " '9.11.2.5': 443,\n",
    " '9.11.3': 444,\n",
    " '9.11.4': 445,\n",
    " '9.2': 446,\n",
    " 'A2': 447,\n",
    " 'A3': 448,\n",
    " 'A4': 449,\n",
    " 'A5': 450,\n",
    " 'A6': 451,\n",
    " 'B-2': 452,\n",
    " 'B-3': 453,\n",
    " 'B-4': 454,\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing dataset\n",
    "sequence_length = 32\n",
    "\n",
    "# sort all unique elements of notes-list\n",
    "pitchnames = sorted(set(item for item in notes))\n",
    "\n",
    "# create a dictionary to map pitches to integers\n",
    "note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "network_input = []\n",
    "\n",
    "#  create input sequences and the corresponding outputs\n",
    "for i in range(0, len(notes) - sequence_length, sequence_length):\n",
    "    sequence_in = notes[i:i + sequence_length] \n",
    "    network_input.append([note_to_int[char] for char in sequence_in])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing input, and creating upscaler for later\n",
    "\n",
    "I get the max value from the network input.\n",
    "\n",
    "Then I create the feature range (0,max network input) for a scaler from *sklearn.preprocessing.MinMaxScaler*.\n",
    "\n",
    "And I use the max value to normalize the network_input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get max value from network input\n",
    "maxr = max(max(network_input))\n",
    "\n",
    "# create feature range for upscaler\n",
    "feature_range = (0,maxr)\n",
    "# prepare scaler for later\n",
    "predictScaler = MinMaxScaler(feature_range=feature_range)\n",
    "\n",
    "# saving feature range, useful elsewhere\n",
    "np.save(\"feature_range.npy\", feature_range)\n",
    "           \n",
    "\n",
    "# normalize input\n",
    "network_input = np.asarray(network_input)\n",
    "normalized_input = network_input / maxr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating train and test set\n",
    "\n",
    "I split the network_input by 2/3 to my training set, and keep the last 1/3 for my test set. \n",
    "Then I save it for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11292, 32), (5645, 32))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split\n",
    "split_point = int(normalized_input.shape[0] * 2 / 3)\n",
    "\n",
    "x_train, x_test = normalized_input[0:split_point,:], normalized_input[split_point:-1,:]\n",
    "\n",
    "np.savez(\"music.npz\", x_train=x_train,x_test=x_test)\n",
    "\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the network\n",
    "\n",
    "The weights are initalized with random normal distribution, as this keeps them close to the dataset. The relu actvation function gives the best result. All the values in the network are positive, so that's not a surprise. \n",
    "And relu prevents vanishing gradients. I experienced slow convergence with sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of encoded representation\n",
    "input_dim = network_input.shape[1]\n",
    "\n",
    "# input placeholder\n",
    "input_song = Input(shape=(input_dim,))\n",
    "\n",
    "# encoder\n",
    "encoded = Dense(encoding_dim, kernel_initializer='random_normal',\n",
    "               bias_initializer='zeros', activation='relu')(input_song)\n",
    "\n",
    "#decoder\n",
    "decoded = Dense(input_dim, kernel_initializer='random_normal',\n",
    "                bias_initializer='zeros', activation='relu')(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The autoencoder maps the input to its reconstruction\n",
    "# input=input song, output = decoded song\n",
    "\n",
    "autoencoder = Model(input_song, decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The encoder and decoder\n",
    "\n",
    "These aren't really necessary for making predictions in this example. It just exemplifies that encoding and decoding can be broken down to separate models and trained. I don't use it for making predictions later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate encoder model\n",
    "encoder = Model(input_song, encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate decoder model\n",
    "\n",
    "# create placeholder for encoded (32 dim) input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "\n",
    "# retrieve last layer of autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "\n",
    "# make decoder model\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "The model is in practice trying to estimate the distance between calculated input and true input, and these are not likelihood estimations, but just number representations. Therefore I chose *mean squared error* as a loss function.\n",
    "\n",
    "Chose rmsprop as I knew it was a good optimizer, gives better result than adam and adadelta. But, I don't have a good explanation at the moment.\n",
    "\n",
    "Use earlystopping with a patience of 20 epochs, and minimum change 10e-5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7904 samples, validate on 3388 samples\n",
      "Epoch 1/400\n",
      "7904/7904 [==============================] - 0s 27us/step - loss: 0.1093 - val_loss: 0.1081\n",
      "Epoch 2/400\n",
      "7904/7904 [==============================] - 0s 11us/step - loss: 0.1092 - val_loss: 0.1081\n",
      "Epoch 3/400\n",
      "7904/7904 [==============================] - 0s 9us/step - loss: 0.1091 - val_loss: 0.1081\n",
      "Epoch 4/400\n",
      "7904/7904 [==============================] - 0s 9us/step - loss: 0.1091 - val_loss: 0.1081\n",
      "Epoch 5/400\n",
      "7904/7904 [==============================] - 0s 7us/step - loss: 0.1091 - val_loss: 0.1081\n",
      "Epoch 6/400\n",
      "7904/7904 [==============================] - 0s 8us/step - loss: 0.1091 - val_loss: 0.1081\n",
      "Epoch 7/400\n",
      "7904/7904 [==============================] - 0s 11us/step - loss: 0.1091 - val_loss: 0.1081\n",
      "Epoch 8/400\n",
      "7904/7904 [==============================] - 0s 12us/step - loss: 0.1091 - val_loss: 0.1080\n",
      "Epoch 9/400\n",
      "7904/7904 [==============================] - 0s 10us/step - loss: 0.1091 - val_loss: 0.1081\n",
      "Epoch 10/400\n",
      "7904/7904 [==============================] - 0s 9us/step - loss: 0.1091 - val_loss: 0.1080\n",
      "Epoch 11/400\n",
      "7904/7904 [==============================] - 0s 13us/step - loss: 0.1091 - val_loss: 0.1081\n",
      "Epoch 12/400\n",
      "7904/7904 [==============================] - 0s 9us/step - loss: 0.1091 - val_loss: 0.1080\n",
      "Epoch 13/400\n",
      "7904/7904 [==============================] - 0s 13us/step - loss: 0.1091 - val_loss: 0.1081\n",
      "Epoch 14/400\n",
      "7904/7904 [==============================] - 0s 10us/step - loss: 0.1090 - val_loss: 0.1081\n",
      "Epoch 15/400\n",
      "7904/7904 [==============================] - 0s 12us/step - loss: 0.1091 - val_loss: 0.1081\n",
      "Epoch 16/400\n",
      "7904/7904 [==============================] - 0s 10us/step - loss: 0.1090 - val_loss: 0.1080\n",
      "Epoch 17/400\n",
      "7904/7904 [==============================] - 0s 12us/step - loss: 0.1090 - val_loss: 0.1081\n",
      "Epoch 18/400\n",
      "7904/7904 [==============================] - 0s 9us/step - loss: 0.1090 - val_loss: 0.1080\n",
      "Epoch 19/400\n",
      "7904/7904 [==============================] - 0s 14us/step - loss: 0.1090 - val_loss: 0.1080\n",
      "Epoch 20/400\n",
      "7904/7904 [==============================] - 0s 12us/step - loss: 0.1090 - val_loss: 0.1081\n",
      "Epoch 21/400\n",
      "7904/7904 [==============================] - 0s 10us/step - loss: 0.1090 - val_loss: 0.1079\n",
      "Epoch 22/400\n",
      "7904/7904 [==============================] - 0s 11us/step - loss: 0.1090 - val_loss: 0.1079\n",
      "Epoch 23/400\n",
      "7904/7904 [==============================] - 0s 11us/step - loss: 0.1090 - val_loss: 0.1080\n",
      "Epoch 24/400\n",
      "7904/7904 [==============================] - 0s 9us/step - loss: 0.1090 - val_loss: 0.1079\n",
      "Epoch 25/400\n",
      "7904/7904 [==============================] - 0s 10us/step - loss: 0.1089 - val_loss: 0.1080\n",
      "Epoch 26/400\n",
      "7904/7904 [==============================] - 0s 8us/step - loss: 0.1089 - val_loss: 0.1080\n",
      "Epoch 27/400\n",
      "7904/7904 [==============================] - 0s 10us/step - loss: 0.1089 - val_loss: 0.1080\n",
      "Epoch 28/400\n",
      "7904/7904 [==============================] - 0s 13us/step - loss: 0.1089 - val_loss: 0.1080\n",
      "Epoch 29/400\n",
      "7904/7904 [==============================] - 0s 11us/step - loss: 0.1089 - val_loss: 0.1080\n",
      "Epoch 30/400\n",
      "7904/7904 [==============================] - 0s 10us/step - loss: 0.1089 - val_loss: 0.1080\n",
      "Epoch 31/400\n",
      "7904/7904 [==============================] - 0s 8us/step - loss: 0.1089 - val_loss: 0.1080\n",
      "Epoch 32/400\n",
      "7904/7904 [==============================] - 0s 11us/step - loss: 0.1089 - val_loss: 0.1079\n",
      "Epoch 00032: early stopping\n",
      "time to train 3.037297248840332\n"
     ]
    }
   ],
   "source": [
    "# use per-pixel binary crossentropy-loss and Adadelta optimizer\n",
    "autoencoder.compile(optimizer='rmsprop', loss='mean_squared_error')\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=10e-5, patience=20,\n",
    "                          verbose=1, mode='auto')\n",
    "\n",
    "callbacks_list = [earlystop]\n",
    "\n",
    "# train the model\n",
    "start = time.time()\n",
    "\n",
    "model_info = autoencoder.fit(x_train, x_train, \n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                callbacks=callbacks_list,\n",
    "                validation_split=0.3)\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"time to train\", end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare to play music\n",
    "\n",
    "Read and borrowed parts from [this](https://towardsdatascience.com/how-to-generate-music-using-a-lstm-neural-network-in-keras-68786834d4c5), but I changed the code to include note rests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all pitch names\n",
    "int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "\n",
    "# This is just necessary to save the dictionary for use elsewhere\n",
    "# It's not possible to save a dictionary object, but lists are no problem\n",
    "# I zip them back to a dictionary when needed\n",
    "keys = list(int_to_note.keys())\n",
    "values = list(int_to_note.values())\n",
    "np.savez(\"int_to_note.npz\", keys=keys, values=values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createPattern(input_sequence):\n",
    "    \"\"\"\n",
    "    Function that map integers from note_to_int-dictionary\n",
    "    back to string representation of notes and chords.\n",
    "    \n",
    "    Input: sequence of 32 integers representing a short song\n",
    "    \n",
    "    Output: Note and chord representations as strings\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    prediction_output = []\n",
    "\n",
    "    # generate notes\n",
    "    for note_index in input_sequence:\n",
    "                \n",
    "        result = int_to_note[note_index]\n",
    "        prediction_output.append(result)\n",
    "    return prediction_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a prediction\n",
    "decoded_song = autoencoder.predict(x_test)\n",
    "\n",
    "# rescaling the result to fit embedding\n",
    "song = (predictScaler.fit_transform(decoded_song)).astype('int')\n",
    "\n",
    "# choose a sequence\n",
    "newsong = createPattern(song[40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create music21 stream\n",
    "\n",
    "I'm creating a miditrack where each note has an offset of 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = midi.MidiTrack(0)\n",
    "dt = midi.DeltaTime(mt)\n",
    "dt.time = 0.5\n",
    "s1 = stream.Stream()\n",
    "\n",
    "for item in newsong:\n",
    "    \n",
    "    if ('.' in item) or item.isdigit():\n",
    "        # chord\n",
    "        notes_in_chord = item.split('.')\n",
    "        notes = []\n",
    "\n",
    "        for current_note in notes_in_chord:\n",
    "            new_note = note.Note(int(current_note))\n",
    "            new_note.storedInstrument = instrument.Guitar()\n",
    "            notes.append(new_note)\n",
    "\n",
    "        new_chord = chord.Chord(notes)\n",
    "        s1.append(new_chord)\n",
    "\n",
    "    elif item is not '' and ('.' not in item):\n",
    "        # notes\n",
    "        new_note = note.Note(item)\n",
    "        s1.append(new_note)\n",
    "\n",
    "\n",
    "    elif item == '':\n",
    "        # rest\n",
    "        s1.append(note.Rest())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = midi.realtime.StreamPlayer(s1)\n",
    "\n",
    "sp.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare test set and decoded test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.75711864, 0.8293800943207219, 0.29313558, 0.26314332054883544)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_song.mean(), x_test.mean(), decoded_song.std(), x_test.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magenta3",
   "language": "python",
   "name": "magenta3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
